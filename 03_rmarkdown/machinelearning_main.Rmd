---
title: "Machine Learning I Group Work"
author: "Adriana Ricklin, Yvonne Schaerli, Christina Sudermann, Carole Mattmann"
date: "11 June 2020"
output:
  html_document: 
    toc: true
    number_sections: true
  pdf_document: 
    toc: true
    number_sections: true
---


```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)

```

# Packages
```{r message=FALSE, warning=FALSE, paged.print=FALSE}
library(gridExtra)
library(tidyverse) # inclued ggplot2
library(e1071)     # for SVM

```


# Import and data cleaning

```{r}
insurance <- read.csv("../01_data/insurance.csv", header=TRUE)
str(insurance)


# smoker = 1 / nonsmoker = 0
insurance$smoker <- as.character(insurance$smoker)
insurance$smoker[insurance$smoker == "yes"] <- "1"
insurance$smoker[insurance$smoker == "no"] <- "0"
insurance$smoker <- as.factor(insurance$smoker)


head(insurance)

```



# Linear models -> Christina

```{r}

```

# Linear models (GAM & Polynomial) -> Yvonne 

```{r}

```

# GLM and cross validation -> Carole 

## Generalised Linear Models for count data

### Original data

The number of children an insured person has is analysed. We have the following data on children per person. The number of children ranges from 0 to 5 with a median of 1. 

```{r}
summary(insurance$children)
```

### Poisson model 

To model count data (number of children) the poisson model is used. An analysis performed beforehand showed that only the variables "charges" and "smoker" have a significant impact on the number of children. 

```{r}

glm.children <- glm(children ~ smoker+charges, 
                   data=insurance, 
                   family = "poisson")

summary(glm.children)

```

To get the coefficients, the log transformation needs to be reversed:

```{r}
exp(coef(glm.children)) 
```

Smoker (factor): The model shows that for the factor smoker (yes/no), a smoker has on average 72% of the number of children a non-smoker has. The more common-sense interpretation might be the other way around, that people who have 1 or more children smoke less, but for the moment we have no proof of that. 

Charges: A person with higher charges will on average have more children. If charges are increased by 1000 dollars, the calculated number of children increases by 1.4%. 

```{r eval=FALSE, include=FALSE}
coef(glm.children)["charges"]*1000
```

### Simulation of data and comparison

With the calculated model, data is simulated:

```{r echo=FALSE}
set.seed(5)
sim.data.glm.children <- simulate(glm.children)
summary(sim.data.glm.children)
```

The original and the simulated data are compared visually. The number of children from the simulated data (0-6) seem to be plausible. The distribution has a strong downwards trend starting at 1 like the original data. However the model does not seem to generate enough data with 0 children. 

```{r echo=FALSE}
df <- data.frame(insurance)
plot.original <- ggplot(df, 
                        aes(x=children))+ 
  geom_bar(fill="darkgreen")+
  labs(title = "Original data", x="number of children")

plot.sim <- ggplot(sim.data.glm.children, 
                   aes(x=sim_1))+
                     geom_bar(fill="darkblue")+
  labs(title = "Simulated data", x="number of children")

grid.arrange(plot.original, plot.sim, nrow=1)
```


## Generalised Linear Models for binomial data 


A model is fitted that predicts if a person is a smoker or not. Only the significant values age, bmi and charges are used. 

```{r}
glm.smoker.2 <- glm(smoker ~ age+bmi+charges, 
                    data=insurance, 
                    family = "binomial")

summary(glm.smoker.2)

exp(coef(glm.smoker.2)) 
```

Age and BMI has a negative effect on smoker. This means the higher a persons BMI and age, the lower the probability that the person is a smoker. 
Charges has a positive effect. This means the higher a persons charges, the higher is
the possibility that the person smokes.

### Graphical analysis

This can also be explored graphically, at least for charges it is clearly visible that smokers have higher charges. 

```{r echo=FALSE, message=FALSE, warning=FALSE}

plot1 <- ggplot(data = insurance,
       mapping = aes(y = smoker,
                     x = age)) +
  geom_point(alpha = 0.05)

plot2 <- ggplot(data = insurance,
       mapping = aes(y = smoker,
                     x = bmi)) +
  geom_point(alpha = 0.05)

plot3 <- ggplot(data = insurance,
       mapping = aes(y = smoker,
                     x = charges)) +
  geom_point(alpha = 0.05)

grid.arrange(plot1, plot2, plot3, ncol=3)
```

### Estimating the model performance

The predicted values are transformed into binary (beforehand they inicated the probability) and compared with the actual data. 

```{r echo=TRUE, message=FALSE, warning=FALSE}

fitted.smoker.disc <- ifelse(fitted(glm.smoker.2) < 0.5,
                           yes = 0, no = 1)
head(fitted.smoker.disc)

d.obs.fit.smoker <- data.frame(obs = insurance$smoker,
                             fitted = fitted.smoker.disc)
head(d.obs.fit.smoker)
```

We observe the following fit:

```{r echo=FALSE, message=FALSE, warning=FALSE}

table(obs = d.obs.fit.smoker$obs,
      fit = d.obs.fit.smoker$fitted)
```

## Cross Validation 

Three linear models are cross validated:

```{r}
lm.1 <- lm(data=insurance, charges~children+smoker+bmi+age+region+sex)

lm.2 <- lm(data=insurance, charges~children+smoker)

lm.3 <- lm(data=insurance, charges~(poly(bmi, degree=3))
           +(poly(age, degree=2))+children+smoker)
```

The in sample performance, using R Squared as measure is the following: 

```{r}
summary(lm.1)$r.squared
summary(lm.2)$r.squared
summary(lm.3)$r.squared
```

The out of sample performance is computed by using 50:50 training and test data and repeating the process 100 times. 

```{r}
set.seed(5)

r.squared.lm.1 <- c()
r.squared.lm.2 <- c()
r.squared.lm.3 <- c()

for(i in 1:100){
  
  # prepare data
  
  train.YES <- sample(x=c(TRUE,FALSE), 
                      size=nrow(insurance), 
                      replace = TRUE)
  
  table(train.YES)
  
  insurance.train <- insurance[train.YES, ]
  insurance.test <- insurance[!train.YES, ]
  
  # fit model with train data 
  
  lm.1.train <- lm(formula = formula(lm.1), 
                   data = insurance.train)
  
  lm.2.train <- lm(formula = formula(lm.2), 
                   data = insurance.train)
  
  lm.3.train <- lm(formula = formula(lm.3), 
                   data = insurance.train)
  
  # make prediction on test data
  lm.1.predict <- predict(lm.1.train, 
                          newdata = insurance.test)
  
  lm.2.predict <- predict(lm.2.train, 
                          newdata = insurance.test)
  
  lm.3.predict <- predict(lm.3.train, 
                          newdata = insurance.test)
  
  # compute r.squared and save in list
  
  r.squared.lm.1[i] <- cor(lm.1.predict, 
                           insurance.test$charges)^2
  
  r.squared.lm.2[i] <- cor(lm.2.predict, 
                           insurance.test$charges)^2
  
  r.squared.lm.3[i] <- cor(lm.3.predict, 
                           insurance.test$charges)^2
}

```

The out of sample performance, using R Squared as measure is the following: 

```{r}
#lm.1 
mean(r.squared.lm.1)
#lm.2
mean(r.squared.lm.2)
#lm.3
mean(r.squared.lm.3)
```

```{r}
par(mfrow=c(1,3))
boxplot(r.squared.lm.1, main="lm.1")
boxplot(r.squared.lm.2, main="lm.2")
boxplot(r.squared.lm.3, main="lm.3")
```
It can be observed that lm.3 performs slightly better but it is also the most complicated. lm.1 might be the better model, the performance is just slightly lower and it is simpler. 

# Decision Trees -> Adriana

# Support Vector Machines -> Carole

In this chapter, Support Vector Machines are applied to the insurance dataset. 

## two classes linear

By visual exploration, we discover that smokers and non-smokers form two groups. We fit an SVM for those two groups. The lines in the graph are the first guess of how the data could be divided.

```{r}


# guess of hyperplane
slope <- 0.001
intercept <- 10

ggplot(data = insurance) +
  geom_point(aes(x = charges, y = bmi, color=smoker))+
  geom_abline(slope = slope, intercept = intercept)


```


A linear SVM is tuned using different cost ranges. 

```{r}
set.seed(5)
#get optimal cost 
cost_range <-
  c(1e-10,
    1e-7,
    1e-5,
    0.001,
    0.0025,
    0.005,
    0.0075,
    0.01,
    0.1,
    1,
    5,
    10,
    100, 
    200)

tune.out.1 <- tune(
  svm,
  smoker ~ charges+bmi,
  data = insurance.train,
  kernel = "linear",
  ranges = list(cost = cost_range)
)

```

The following is the best model. A cost of 5 is used. There are 104 support vectors.

```{r}
#best model 
bestmod <- tune.out.1$best.model
summary(bestmod)
plot(bestmod, insurance.train, bmi~charges)

```

The confision matrices and the classifiaction error rate for train and test data are:

```{r}
# confusion matrix train
confusion.lin.train <- table(predict = predict(bestmod, insurance.train),
      truth = insurance.train$smoker)
confusion.lin.train

#classification error rate train
(confusion.lin.train[1,2]+confusion.lin.train[2,1])/sum(confusion.lin.train[1:2,1:2])

```

```{r}
# confusion matrix linear test
confusion.lin.test <- table(predict = predict(bestmod, insurance.test),
      truth = insurance.test$smoker)
confusion.lin.test

#classification error rate test
(confusion.lin.test[1,2]+confusion.lin.test[2,1])/sum(confusion.lin.test[1:2,1:2])

```

## two classes polynomial

A polynomial model was also tested but the classification error rate was slightly higher: 

```{r}
set.seed(5)
#get optimal cost and degree for polynomial

cost_range <-
  c(1e-10,
    1e-7,
    1e-5,
    0.001,
    0.0025,
    0.005,
    0.0075,
    0.01,
    0.1,
    1,
    5,
    10,
    100)

degree_range <- 2:4

tune.out.poly <- tune(
  svm,
  smoker ~ charges+bmi,
  data = insurance.train,
  kernel = "polynomial",
  ranges = list(cost = cost_range, degree = degree_range))

#best model poly
tune.out.poly$best.parameters
bestmod.poly <- tune.out.poly$best.model
summary(bestmod.poly)
plot(bestmod.poly, insurance.train, bmi~charges)


# confusion matrix 
confusion.poly.train <- table(predict = predict(bestmod.poly, insurance.train),
      truth = insurance.train$smoker)

confusion.poly.test <- table(predict = predict(bestmod.poly, insurance.test),
      truth = insurance.test$smoker)

#classification error rate 

(confusion.poly.train[1,2]+confusion.poly.train[2,1])/sum(confusion.poly.train[1:2,1:2])

(confusion.poly.test[1,2]+confusion.poly.test[2,1])/sum(confusion.poly.test[1:2,1:2])

```



